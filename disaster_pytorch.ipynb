{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Using cached opencv_python-4.5.5.64-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.5 MB)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/anis/anaconda3/envs/ml/lib/python3.8/site-packages (from opencv-python) (1.21.2)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.5.5.64\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python pillow matplotlib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m__future__\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m print_function, division\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageFile\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "\n",
    "img_transform = transforms.Compose([transforms.Resize((224, 224)),transforms.ToTensor(),\n",
    "                                    transforms.Normalize((0.5,), (0.5,))])\n",
    "train_data_dir = datasets.ImageFolder(\"data/train/\", transform = img_transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data_dir, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 1050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /home/anis/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
      "100.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (18): ReLU(inplace=True)\n",
       "      (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (25): ReLU(inplace=True)\n",
       "      (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (27): ReLU(inplace=True)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (1): NeuralNetwork(\n",
       "    (global_avg_pool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "    (dropout): Dropout(p=0.7, inplace=False)\n",
       "    (fc): Linear(in_features=512, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "\n",
    "model_vgg = models.vgg16(pretrained='imagenet')\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.global_avg_pool = nn.AvgPool2d(kernel_size=7)\n",
    "        self.dropout = nn.Dropout(p=0.7)\n",
    "        self.fc = nn.Linear(512, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.global_avg_pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        dim = x.shape[0]\n",
    "        v=x.view(dim,512,-1)\n",
    "        x=v.mean(2)\n",
    "        x=x.view(1, dim, 512)\n",
    "        x = self.fc(x)\n",
    "        return x.view(-1, 8)\n",
    "\n",
    "\n",
    "net = NeuralNetwork()\n",
    "model = nn.Sequential(*list(model_vgg.children())[:-1])\n",
    "model = nn.Sequential(model, net)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.randn(1, 3, 224, 224).cuda()\n",
    "output = model(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_parameters = []\n",
    "for name, p in model.named_parameters():\n",
    "    #print(name)\n",
    "    if \"fc\" in name:\n",
    "        trainable_parameters.append(p)\n",
    "optimizer = torch.optim.SGD(params=trainable_parameters, lr=0.1, momentum=1e-5)  \n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/4], Step [1/774], Loss: 2.3335, Accuracy: 0.00%\n",
      "Epoch [1/4], Step [101/774], Loss: 0.5458, Accuracy: 87.50%\n",
      "Epoch [1/4], Step [201/774], Loss: 0.3680, Accuracy: 87.50%\n",
      "Epoch [1/4], Step [301/774], Loss: 0.8391, Accuracy: 62.50%\n",
      "Epoch [1/4], Step [401/774], Loss: 0.6640, Accuracy: 75.00%\n",
      "Epoch [1/4], Step [501/774], Loss: 1.0499, Accuracy: 62.50%\n",
      "Epoch [1/4], Step [601/774], Loss: 1.4799, Accuracy: 62.50%\n",
      "Epoch [1/4], Step [701/774], Loss: 0.3651, Accuracy: 87.50%\n",
      "Epoch [2/4], Step [1/774], Loss: 1.0342, Accuracy: 62.50%\n",
      "Epoch [2/4], Step [101/774], Loss: 1.3439, Accuracy: 62.50%\n",
      "Epoch [2/4], Step [201/774], Loss: 0.6661, Accuracy: 75.00%\n",
      "Epoch [2/4], Step [301/774], Loss: 0.5179, Accuracy: 87.50%\n",
      "Epoch [2/4], Step [401/774], Loss: 0.6059, Accuracy: 75.00%\n",
      "Epoch [2/4], Step [501/774], Loss: 0.7061, Accuracy: 75.00%\n",
      "Epoch [2/4], Step [601/774], Loss: 0.6539, Accuracy: 75.00%\n",
      "Epoch [2/4], Step [701/774], Loss: 1.1791, Accuracy: 75.00%\n",
      "Epoch [3/4], Step [1/774], Loss: 0.4952, Accuracy: 87.50%\n",
      "Epoch [3/4], Step [101/774], Loss: 0.6283, Accuracy: 62.50%\n",
      "Epoch [3/4], Step [201/774], Loss: 1.4312, Accuracy: 87.50%\n",
      "Epoch [3/4], Step [301/774], Loss: 0.3546, Accuracy: 87.50%\n",
      "Epoch [3/4], Step [401/774], Loss: 0.4548, Accuracy: 75.00%\n",
      "Epoch [3/4], Step [501/774], Loss: 0.1253, Accuracy: 100.00%\n",
      "Epoch [3/4], Step [601/774], Loss: 2.6923, Accuracy: 25.00%\n",
      "Epoch [3/4], Step [701/774], Loss: 0.2657, Accuracy: 100.00%\n",
      "Epoch [4/4], Step [1/774], Loss: 0.4093, Accuracy: 75.00%\n",
      "Epoch [4/4], Step [101/774], Loss: 0.4002, Accuracy: 87.50%\n",
      "Epoch [4/4], Step [201/774], Loss: 0.7367, Accuracy: 87.50%\n",
      "Epoch [4/4], Step [301/774], Loss: 0.4385, Accuracy: 87.50%\n",
      "Epoch [4/4], Step [401/774], Loss: 0.7823, Accuracy: 62.50%\n",
      "Epoch [4/4], Step [501/774], Loss: 0.3844, Accuracy: 87.50%\n",
      "Epoch [4/4], Step [601/774], Loss: 1.0921, Accuracy: 62.50%\n",
      "Epoch [4/4], Step [701/774], Loss: 0.4084, Accuracy: 75.00%\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "min_loss = 9999\n",
    "num_epochs = 4\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass'\n",
    "        \n",
    "        outputs = model(images.cuda())\n",
    "       \n",
    "        loss = criterion(outputs.cuda(), labels.cuda())\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.cuda().data, 1)\n",
    "        correct = (predicted.cuda() == labels.cuda()).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "        \n",
    "        if (i % 100) == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))\n",
    "    \n",
    "    with open('loss.txt', 'w+') as f:\n",
    "        f.write(\"%s\\n\" % loss)\n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        torch.save(model.state_dict(), './checkpoints/training4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88169102_652284975535197_1304077415391166464_n.jpg\n",
      "89307158_652284822201879_6446886657374289920_n.jpg\n",
      "89327718_652284882201873_1111651885414088704_n.jpg\n",
      "89384604_652284915535203_4330144899220373504_n.jpg\n",
      "89441998_652284802201881_5384157520363257856_n.jpg\n",
      "89472885_652284852201876_4462420906865590272_n.jpg\n",
      "89592011_652284945535200_8137987879365246976_n.jpg\n",
      "89616139_652285002201861_1795615248229597184_n.jpg\n",
      "89695157_652284782201883_4420443515717156864_n.jpg\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "model.load_state_dict(torch.load('./checkpoints/training4.pth'))       \n",
    "model.eval()       \n",
    "mod = nn.Sequential(*list(model_vgg.children())[:-1])\n",
    "# features_blobs = []\n",
    "\n",
    "# # def hook_feature(module, input, output):\n",
    "# #     features_blobs.append(output.data.cpu().numpy())\n",
    "\n",
    "# mod.register_forward_hook(hook_feature)\n",
    "\n",
    "# print(features_blobs)   \n",
    "params = list(net.parameters())\n",
    "weight_softmax = np.squeeze(params[-2].data.cpu().numpy())\n",
    "\n",
    "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
    "    # generate the class -activation maps upsample to 256x256\n",
    "    size_upsample = (256, 256)\n",
    "    bz, nc, h, w = feature_conv.shape\n",
    "    \n",
    "    output_cam = []\n",
    "    for idx in class_idx:\n",
    "#         print(weight_softmax[idx])\n",
    "        cam = weight_softmax[idx].dot(feature_conv.reshape((nc, h*w)))\n",
    "        \n",
    "#         print(cam.shape)\n",
    "#         cam = cam.view(49,7)\n",
    "#         print(cam.shape)\n",
    "        \n",
    "        cam = cam.reshape(h, w)\n",
    "        cam = cam - np.min(cam)\n",
    "        cam_img = cam / np.max(cam)\n",
    "        cam_img = np.uint8(255 * cam_img)\n",
    "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
    "    return output_cam\n",
    "\n",
    "normalize = transforms.Normalize(\n",
    "  mean=[0.485, 0.456, 0.406],\n",
    "  std=[0.229, 0.224, 0.225]\n",
    ")\n",
    "preprocess = transforms.Compose([\n",
    "  transforms.Resize((224,224)),\n",
    "  transforms.ToTensor(),\n",
    "  normalize\n",
    "])\n",
    "\n",
    "# list1 = [open('test1.txt','r')]\n",
    "# IMG_URL = list1.readlines()\n",
    "org_loc ='data/fire/'\n",
    "\n",
    "for fname in os.listdir(org_loc):\n",
    "    print(fname)\n",
    "    img_pil = Image.open(org_loc + fname)\n",
    "#     plt.imshow(img_pil)\n",
    "    img_tensor = preprocess(img_pil)\n",
    "    \n",
    "    img_variable = Variable(img_tensor.unsqueeze(0))\n",
    "    img_variable = img_variable.to(device)\n",
    "\n",
    "    logit = model(img_variable)\n",
    "    \n",
    "    h_x = F.softmax(logit, dim=1).data.squeeze()\n",
    "    \n",
    "    probs, idx = h_x.sort(0, True)\n",
    "    probs = probs.cpu().detach().numpy()\n",
    "    idx = idx.cpu().numpy()\n",
    "    \n",
    "    features_blobs = mod(img_variable)\n",
    "    features_blobs1 = features_blobs.cpu().detach().numpy()\n",
    "    CAMs = returnCAM(features_blobs1, weight_softmax, [idx[0]])\n",
    "    \n",
    "    img = cv2.imread(org_loc + fname)\n",
    "    \n",
    "    height, width, _ = img.shape\n",
    "    \n",
    "    heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
    "    result = heatmap * 0.3 + img * 0.5\n",
    "    \n",
    "    cv2.imwrite('./results/test3/' + fname, result) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
